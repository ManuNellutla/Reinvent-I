---
title: "Confusion Matrix"
author: "Manu Nellutla"
date: "8/26/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Confusion Matrix

Measuring a model by accuracy can be misleading as the data used to train and test can be baised or have a uneven representation of the desired population mix.

To address this issue we often use **Confusion Matrix** to validate our models performance based on few additional metrics. But first a confusion matrix

<br/>
<style>
table, th, td {
  align: center;
  border: 1px solid black;
  padding: 5px;
}
td{
  width:25%;
}
th{
 background-color: lightgray;
}
.center {
  margin-left: auto;
  margin-right: auto;
}

.blue{
 background-color:lightblue;
}
</style>

<table width="500" class="center">
<tr>
  <th rowspan=2 colspan=2>$$n= TP+TN+FP+FN$$</th>
  <th colspan=2> <center>Actual<center></th>
</tr>
<tr>
  <td class="blue"><b>Yes</b></td>
  <td class="blue"><b>No</b></td>
</tr>
<tr>
  <th rowspan=2> Predicted</th>
  <td class="blue"><b>Yes</b></td>
  <td>TP</td>
  <td>FP</td>
</tr>
 <td class="blue"><b>No</b></td>
  <td>FN</td>
  <td>TN</td>
<tr>

</tr>
</table>

<br/>


```{r include=FALSE}
cf <- matrix(c('TP','FN','FP','TN'), nrow=2, ncol=2)
cf
cf_df <- as.data.frame(cf,row.names = c('Yes','No'))
names(cf_df)[1] <- 'Yes'
names(cf_df)[2] <- 'No'
knitr::kable(cf_df)
```


### Intro


    population (n) = all rows = (TP + FP + TN + FN)


- $Accuracy = \frac{(TP + TN)}{(TP + TN + FP + FN)}$ - proportion of correct classifications.

- $Precision = \frac{TP}{TP+FP}$ - This is also called *'Pos Prediction'*

- $Recall = \frac{TP}{TP+FN}$ - This is also called *'Sensitivity'*


### load data

```{r 'Get Data'}
library(dslabs)
data(heights)
summary(heights)
```

### split dataset

Using **caret.createDataPartion**
[https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/createDataPartition]

```{r 'split dataset'}
library("caret")
y = heights$sex
X = heights$height
set.seed(2, sample.kind = "Rounding")
# split at 0,5 
test_index = createDataPartition(y, times = 1, p = 0.5, list=FALSE)

test_set = heights[test_index,] 
train_set = heights[-test_index,]

summary(test_set)

summary(train_set)

```


### predic with best accuracy

Now that we know the best cutoff height is 64 lets predict with new cutoff

```{r}
library(purrr)
y_hat <- ifelse(test_set$height > 64, "Male", "Female") %>% factor(levels = levels(test_set$sex))
y_hat <- factor(y_hat)
mean(y_hat == test_set$sex)
```

### confusion matrix

```{r}
confusionMatrix(data=y_hat, reference=test_set$sex)
```

### Understanding results

If you look at the results we understand that **accuracy is about 81%**. 

That looks good. But is it?

If you look at the "**Pos Prediction above it is 64%**". This is our actual precision which tells us that proportion of 'True Positives' are low.

**Sensitivity/ Recall** is the proportion of correct classification for a class with in the population. 
> *In the above table*: predicted female = 50; actual female = 119 (look at test set summary)
  
Why is this? it is because of the cutoff we chose. I chose to classify male if height > 64. However, the mean of female height is 65. This makes the model intutively wrong.

But 'Male' height cutoff was calculated as $(\mu-2*sd)$ which is right based on our 95% CI. 

What we can infer is that the **PREVALANCE** of the dataset, which means proportion of one class within the population, is uneven.. 

# Maximize F1 score

One preferred metric is **balanced accuracy**. Because specificity and sensitivity are rates, it is more appropriate to compute the harmonic average. In fact, the F1-score, a widely used one-number summary, is the harmonic average of precision and recall. 

$$F1 = \frac{2*[precision*recall]}{[precision+recall]}$$

```{r}
# maximize F-score
cutoff <- seq(61, 70)
F_1 <- map_dbl(cutoff, function(x){
  y_hat <- ifelse(train_set$height > x, "Male", "Female") %>% 
    factor(levels = levels(test_set$sex))
  F_meas(data = y_hat, reference = factor(train_set$sex))
})

F_1
max(F_1)
```

For the same cutoff we find F1 score using "**F_means()**" function of *caret* package. Max F1 score is around 61%.


## lets plot of F1 scores

```{r}
data.frame(cutoff, F_1) %>% 
  ggplot(aes(cutoff, F_1)) + 
  geom_point() + 
  geom_line()

max(F_1)

best_cutoff <- cutoff[which.max(F_1)]
best_cutoff
```
From the R console we see the best cutoff is not at 66 and not 64. This is better as the mean(female_heights) is 65.


## lets check our metrics again for best cutoff

```{r}
y_hat <- ifelse(test_set$height > best_cutoff, "Male", "Female") %>% 
  factor(levels = levels(test_set$sex))
sensitivity(data = y_hat, reference = test_set$sex)
specificity(data = y_hat, reference = test_set$sex)
```

what about confusion matrix

```{r}
confusionMatrix(data=y_hat, reference=test_set$sex)
```
 Lets compare before and after
 
 <table>
 <tr><th>Without F1</th><th>With F1</th></tr>
 <tr><td>
 <pre>
Confusion Matrix and Statistics

          Reference
Prediction Female Male
    Female     50   27
    Male       69  379
                                          
               Accuracy : 0.8171          
                 95% CI : (0.7814, 0.8493)
    No Information Rate : 0.7733          
    P-Value [Acc > NIR] : 0.008354        
                                          
                  Kappa : 0.4041          
                                          
 Mcnemar's Test P-Value : 2.857e-05       
                                          
            Sensitivity : 0.42017         
            Specificity : 0.93350         
         Pos Pred Value : 0.64935         
         Neg Pred Value : 0.84598         
             Prevalence : 0.22667         
         Detection Rate : 0.09524         
   Detection Prevalence : 0.14667         
      Balanced Accuracy : 0.67683         
                                          
       'Positive' Class : Female
  </pre></td>
<td>
<pre>
Confusion Matrix and Statistics

          Reference
Prediction Female Male
    Female     81   67
    Male       38  339
                                          
               Accuracy : 0.8             
                 95% CI : (0.7632, 0.8334)
    No Information Rate : 0.7733          
    P-Value [Acc > NIR] : 0.078192        
                                          
                  Kappa : 0.4748          
                                          
 Mcnemar's Test P-Value : 0.006285        
                                          
            Sensitivity : 0.6807          
            Specificity : 0.8350          
         Pos Pred Value : 0.5473          
         Neg Pred Value : 0.8992          
             Prevalence : 0.2267          
         Detection Rate : 0.1543          
   Detection Prevalence : 0.2819          
      Balanced Accuracy : 0.7578          
                                          
       'Positive' Class : Female 
       
</pre>
</td></tr>
</table>

## ROC Curves

```{r 'ROC Curves'}
library(dplyr)
cutoffs <- c(50, seq(60, 75), 80)
height_cutoff <- map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Female", "Male"))
   list(method = "Height cutoff",
        FPR = 1-specificity(y_hat, test_set$sex),
        TPR = sensitivity(y_hat, test_set$sex))
})
```

### Plotting the ROC curve

A widely used plot that does this is the **receiver operating characteristic (ROC)** curve. The ROC curve plots *sensitivity (TPR)* versus *1 - specificity or the false positive rate (FPR)*.

```{r 'roc curve'}
# plot both curves together
height_cutoff %>%
  ggplot(aes(FPR, TPR, color = method)) +
  geom_line() +
  geom_point() +
  xlab("1 - Specificity") +
  ylab("Sensitivity")
```


### another plot

```{r}
library(ggrepel)
map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Female", "Male"))
   list(method = "Height cutoff",
        cutoff = x, 
        FPR = 1-specificity(y_hat, test_set$sex),
        TPR = sensitivity(y_hat, test_set$sex))
}) %>%
  ggplot(aes(FPR, TPR, label = cutoff)) +
  geom_line() +
  geom_point() +
  geom_text_repel(nudge_x = 0.01, nudge_y = -0.01)
```

## Precision-Recall plot.

However, ROC curves have one weakness and it is that neither of the measures plotted depend on prevalence. In cases in which prevalence matters, we may instead make a precision-recall plot, which has a similar idea with ROC curve.

```{r 'Precision Recall'}
probs <- seq(0, 1, length.out = 10)
guessing <- map_df(probs, function(p){
  y_hat <- sample(c("Male", "Female"), length(test_index), 
                  replace = TRUE, prob=c(p, 1-p)) %>% 
    factor(levels = c("Female", "Male"))
  list(method = "Guess",
    recall = sensitivity(y_hat, test_set$sex),
    precision = precision(y_hat, test_set$sex))
})

height_cutoff <- map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Female", "Male"))
  list(method = "Height cutoff",
       recall = sensitivity(y_hat, test_set$sex),
    precision = precision(y_hat, test_set$sex))
})

bind_rows(guessing, height_cutoff) %>%
  ggplot(aes(recall, precision, color = method)) +
  geom_line() +
  geom_point()
```

### with relevel

Meaning ... If we change positives to mean male instead of females.

```{r 'Another Plot'}
guessing <- map_df(probs, function(p){
  y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE, 
                  prob=c(p, 1-p)) %>% 
    factor(levels = c("Male", "Female"))
  list(method = "Guess",
    recall = sensitivity(y_hat, relevel(test_set$sex, "Male", "Female")),
    precision = precision(y_hat, relevel(test_set$sex, "Male", "Female")))
})

height_cutoff <- map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Male", "Female"))
  list(method = "Height cutoff",
       recall = sensitivity(y_hat, relevel(test_set$sex, "Male", "Female")),
    precision = precision(y_hat, relevel(test_set$sex, "Male", "Female")))
})
bind_rows(guessing, height_cutoff) %>%
  ggplot(aes(recall, precision, color = method)) +
  geom_line() +
  geom_point()
```

