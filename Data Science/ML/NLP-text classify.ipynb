{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using spaCy NLP package.\n",
    "\n",
    "- Author : Manu Nellutla\n",
    "- Date   : Aug 16,2020\n",
    "\n",
    "We will be using SPACY package to classify text and also do a sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/anaconda3/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (46.1.3.post20200330)\n",
      "Requirement already satisfied: thinc==7.4.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (7.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (4.44.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "# install the required packages\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal split of text - \n",
    "\n",
    "**using programmatical split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neuro-linguistic',\n",
       " 'programming',\n",
       " 'was',\n",
       " 'developed',\n",
       " 'in',\n",
       " 'the',\n",
       " \"1970's\",\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'California,',\n",
       " 'Santa',\n",
       " 'Cruz.',\n",
       " 'Its',\n",
       " 'primary',\n",
       " 'founders',\n",
       " 'are',\n",
       " 'John',\n",
       " 'Grinder,',\n",
       " 'a',\n",
       " 'linguist,',\n",
       " 'and',\n",
       " 'Richard',\n",
       " 'Bandler,',\n",
       " 'an',\n",
       " 'information',\n",
       " 'scientist',\n",
       " 'and',\n",
       " 'mathematician.',\n",
       " 'Judith',\n",
       " 'DeLozier',\n",
       " 'and',\n",
       " 'Leslie',\n",
       " 'Cameron-Bandler',\n",
       " 'also',\n",
       " 'contributed',\n",
       " 'significantly',\n",
       " 'to',\n",
       " 'the',\n",
       " 'field,',\n",
       " 'as',\n",
       " 'did',\n",
       " 'David',\n",
       " 'Gordon',\n",
       " 'and',\n",
       " 'Robert',\n",
       " 'Dilts.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text to analyze. and lets split it into words.\n",
    "\n",
    "text =\"Neuro-linguistic programming was developed in the 1970's at the University of California, Santa Cruz. Its primary founders are John Grinder, a linguist, and Richard Bandler, an information scientist and mathematician. Judith DeLozier and Leslie Cameron-Bandler also contributed significantly to the field, as did David Gordon and Robert Dilts.\"\n",
    "\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you see above you can see apostrophe and commas are included in the words and split was based on blanks between the words.\n",
    "\n",
    "## lets check how spaCy does it.\n",
    "\n",
    "**using NLP englis spit**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz#egg=en_core_web_sm==2.3.0 in /usr/local/anaconda3/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.18.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.23.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (46.1.3.post20200330)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (4.44.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import sys\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Neuro',\n",
       " '-',\n",
       " 'linguistic',\n",
       " 'programming',\n",
       " 'was',\n",
       " 'developed',\n",
       " 'in',\n",
       " 'the',\n",
       " '1970',\n",
       " \"'s\",\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'California',\n",
       " ',',\n",
       " 'Santa',\n",
       " 'Cruz',\n",
       " '.',\n",
       " 'Its',\n",
       " 'primary',\n",
       " 'founders',\n",
       " 'are',\n",
       " 'John',\n",
       " 'Grinder',\n",
       " ',',\n",
       " 'a',\n",
       " 'linguist',\n",
       " ',',\n",
       " 'and',\n",
       " 'Richard',\n",
       " 'Bandler',\n",
       " ',',\n",
       " 'an',\n",
       " 'information',\n",
       " 'scientist',\n",
       " 'and',\n",
       " 'mathematician',\n",
       " '.',\n",
       " 'Judith',\n",
       " 'DeLozier',\n",
       " 'and',\n",
       " 'Leslie',\n",
       " 'Cameron',\n",
       " '-',\n",
       " 'Bandler',\n",
       " 'also',\n",
       " 'contributed',\n",
       " 'significantly',\n",
       " 'to',\n",
       " 'the',\n",
       " 'field',\n",
       " ',',\n",
       " 'as',\n",
       " 'did',\n",
       " 'David',\n",
       " 'Gordon',\n",
       " 'and',\n",
       " 'Robert',\n",
       " 'Dilts',\n",
       " '.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary packages\n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp_lang = spacy.load('en')\n",
    "\n",
    "#spacy will use English language annotaitons to split the text\n",
    "\n",
    "nlp_split = nlp_lang(text)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in nlp_split:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "display(len(token_list))\n",
    "token_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split is completely different to what we saw in the normal split. Before using this... \n",
    "\n",
    "## You can also split by sentences\n",
    "\n",
    "**using sentenceTokenizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Neuro-linguistic programming was developed in the 1970's at the University of California, Santa Cruz.\", 'Its primary founders are John Grinder, a linguist, and Richard Bandler, an information scientist and mathematician.', 'Judith DeLozier and Leslie Cameron-Bandler also contributed significantly to the field, as did David Gordon and Robert Dilts.']\n"
     ]
    }
   ],
   "source": [
    "nlp_lang = English()\n",
    "#nlp_lang.remove_pipe(\"sentencizer\")\n",
    "sbd = nlp_lang.create_pipe('sentencizer')\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp_lang.add_pipe(sbd, last=True)\n",
    "\n",
    "nlp_split = nlp_lang(text)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in nlp_split.sents: #----> nlp_split.sents does split by sentences\n",
    "    token_list.append(token.text)\n",
    "print(token_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ability to split we need to remove words that doesn't provide context. These are called stop words.\n",
    "\n",
    "## Stopwords \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['other', 'towards', 'elsewhere', 'not', 'whereafter', 'used', 'same', 'above', 'twelve', 'full']\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "#Printing the total number of stop words:\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "\n",
    "#Printing first ten stop words:\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets filter all the stop words from our text. \n",
    "\n",
    "use **is_stop == false** to remove all the words that are not \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'number of words before filtering : 61'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'number of words after filtering : 41'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Neuro,\n",
       " -,\n",
       " linguistic,\n",
       " programming,\n",
       " developed,\n",
       " 1970,\n",
       " University,\n",
       " California,\n",
       " ,,\n",
       " Santa,\n",
       " Cruz,\n",
       " .,\n",
       " primary,\n",
       " founders,\n",
       " John,\n",
       " Grinder,\n",
       " ,,\n",
       " linguist,\n",
       " ,,\n",
       " Richard,\n",
       " Bandler,\n",
       " ,,\n",
       " information,\n",
       " scientist,\n",
       " mathematician,\n",
       " .,\n",
       " Judith,\n",
       " DeLozier,\n",
       " Leslie,\n",
       " Cameron,\n",
       " -,\n",
       " Bandler,\n",
       " contributed,\n",
       " significantly,\n",
       " field,\n",
       " ,,\n",
       " David,\n",
       " Gordon,\n",
       " Robert,\n",
       " Dilts,\n",
       " .]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#filter stop words from our text.\n",
    "display(f\"number of words before filtering : {len(nlp_split)}\")\n",
    "words_no_stop = [a for a in nlp_split if a.is_stop == False]\n",
    "\n",
    "display(f\"number of words after filtering : {len(words_no_stop)}\")\n",
    "\n",
    "display(words_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize the words.\n",
    "\n",
    "**using '.lemma_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Neuro: ['Neuro', 'PROPN', 'npadvmod'],\n",
       " -: ['-', 'PUNCT', 'punct'],\n",
       " linguistic: ['linguistic', 'ADJ', 'amod'],\n",
       " programming: ['programming', 'NOUN', 'nsubjpass'],\n",
       " developed: ['develop', 'VERB', 'ROOT'],\n",
       " 1970: ['1970', 'NUM', 'nummod'],\n",
       " University: ['University', 'PROPN', 'pobj'],\n",
       " California: ['California', 'PROPN', 'pobj'],\n",
       " ,: [',', 'PUNCT', 'punct'],\n",
       " Santa: ['Santa', 'PROPN', 'compound'],\n",
       " Cruz: ['Cruz', 'PROPN', 'pobj'],\n",
       " .: ['.', 'PUNCT', 'punct'],\n",
       " primary: ['primary', 'ADJ', 'amod'],\n",
       " founders: ['founder', 'NOUN', 'nsubj'],\n",
       " John: ['John', 'PROPN', 'compound'],\n",
       " Grinder: ['Grinder', 'PROPN', 'attr'],\n",
       " ,: [',', 'PUNCT', 'punct'],\n",
       " linguist: ['linguist', 'ADJ', 'appos'],\n",
       " ,: [',', 'PUNCT', 'punct'],\n",
       " Richard: ['Richard', 'PROPN', 'compound'],\n",
       " Bandler: ['Bandler', 'PROPN', 'conj'],\n",
       " ,: [',', 'PUNCT', 'punct'],\n",
       " information: ['information', 'NOUN', 'compound'],\n",
       " scientist: ['scientist', 'NOUN', 'appos'],\n",
       " mathematician: ['mathematician', 'NOUN', 'conj'],\n",
       " .: ['.', 'PUNCT', 'punct'],\n",
       " Judith: ['Judith', 'PROPN', 'compound'],\n",
       " DeLozier: ['DeLozier', 'PROPN', 'nsubj'],\n",
       " Leslie: ['Leslie', 'PROPN', 'compound'],\n",
       " Cameron: ['Cameron', 'PROPN', 'compound'],\n",
       " -: ['-', 'PUNCT', 'punct'],\n",
       " Bandler: ['Bandler', 'PROPN', 'conj'],\n",
       " contributed: ['contribute', 'VERB', 'ROOT'],\n",
       " significantly: ['significantly', 'ADV', 'advmod'],\n",
       " field: ['field', 'NOUN', 'pobj'],\n",
       " ,: [',', 'PUNCT', 'punct'],\n",
       " David: ['David', 'PROPN', 'compound'],\n",
       " Gordon: ['Gordon', 'PROPN', 'nsubj'],\n",
       " Robert: ['Robert', 'PROPN', 'compound'],\n",
       " Dilts: ['Dilts', 'PROPN', 'conj'],\n",
       " .: ['.', 'PUNCT', 'punct']}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_no_stop_lemma = {ab : [ab.lemma_, ab.pos_, ab.dep_] for ab in words_no_stop}\n",
    "\n",
    "words_no_stop_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets do Entity Detection\n",
    "\n",
    "understanding 'person' date etc...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Neuro, 'ORG', 383),\n",
       " (1970, 'DATE', 391),\n",
       " (the University of California, Santa Cruz, 'ORG', 383),\n",
       " (John Grinder, 'PERSON', 380),\n",
       " (Richard Bandler, 'PERSON', 380),\n",
       " (Judith DeLozier, 'PERSON', 380),\n",
       " (Leslie Cameron-Bandler, 'PERSON', 380),\n",
       " (David Gordon, 'PERSON', 380),\n",
       " (Robert Dilts, 'PERSON', 380)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#identifying entities in text\n",
    "entities=[(i, i.label_, i.label) for i in nlp_split.ents]\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displacy - package helps highlite entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Neuro\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "-linguistic programming was developed in the \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1970\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "'s at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the University of California, Santa Cruz\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". Its primary founders are \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John Grinder\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", a linguist, and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Richard Bandler\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", an information scientist and mathematician. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Judith DeLozier\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Leslie Cameron-Bandler\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " also contributed significantly to the field, as did \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    David Gordon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Robert Dilts\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import displacy\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(nlp_split, style = \"ent\",jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
